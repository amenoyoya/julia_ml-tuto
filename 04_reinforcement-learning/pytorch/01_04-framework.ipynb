{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# つくりながら学ぶ！深層強化学習 PyTorchによる実践プログラミング\n",
    "\n",
    "## 迷路問題\n",
    "\n",
    "前回までで、行動反復法と価値反復法による強化学習アルゴリズムを実装してきた\n",
    "\n",
    "今回は、これらのアルゴリズムを選択して強化学習できるようにフレームワークを導入して汎用化してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# MazeEnv\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`(1, 2, 3, 4, 5, 6, 7, 8, 9)`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`(1, 2, 3, 4)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "1\n",
       "```\n"
      ],
      "text/plain": [
       "# MazeEnv\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`(1, 2, 3, 4, 5, 6, 7, 8, 9)`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`(1, 2, 3, 4)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "1\n",
       "```\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ReinforcementLearning, NaNStatistics, Distributions\n",
    "\n",
    "\"エージェントの行動: 上、右、下、左移動\"\n",
    "@enum MazeAction UP=1 RIGHT DOWN LEFT\n",
    "\n",
    "\"迷路問題の状態: S0(START), S1, ..., S8(GOAL)\"\n",
    "@enum MazeState S0=1 S1 S2 S3 S4 S5 S6 S7 S8\n",
    "\n",
    "\"迷路問題環境モデル\"\n",
    "Base.@kwdef mutable struct MazeEnv <: AbstractEnv\n",
    "    state::Int = Int(S0) # 現在の状態 = エージェント位置\n",
    "    reward::Union{Nothing, <:Number} = nothing # 報酬\n",
    "    state_action_history::Vector{NamedTuple} = [] # エージェントの行動記録\n",
    "end\n",
    "\n",
    "\"状態空間: 環境モデル内のすべての状態\"\n",
    "RLBase.state_space(env::MazeEnv) ::Tuple = Int.(instances(MazeState))\n",
    "\n",
    "\"行動空間: エージェントがとりうるすべての行動\"\n",
    "RLBase.action_space(env::MazeEnv) ::Tuple = Int.(instances(MazeAction))\n",
    "\n",
    "\"現在の状態\"\n",
    "RLBase.state(env::MazeEnv) ::Int = env.state\n",
    "\n",
    "\"報酬: エージェントがゴールしない限り報酬は 0\"\n",
    "RLBase.reward(env::MazeEnv) ::Int = isnothing(env.reward) ? 0 : env.reward\n",
    "\n",
    "\"終了条件: 報酬が得られたとき（エージェントがゴールしたとき）\"\n",
    "RLBase.is_terminated(env::MazeEnv) ::Bool = !isnothing(env.reward)\n",
    "\n",
    "\"環境モデル初期化\"\n",
    "RLBase.reset!(env::MazeEnv) = begin\n",
    "    env.state = Int(S0)\n",
    "    env.reward = nothing\n",
    "end\n",
    "\n",
    "\"1step の実行処理\"\n",
    "(env::MazeEnv)(action::Int) = begin\n",
    "    # 行動記録\n",
    "    push!(env.state_action_history, (state = state(env), action = action))\n",
    "\n",
    "    # 状態遷移表\n",
    "    state_map = Dict(\n",
    "        Int(UP)    => -3,\n",
    "        Int(RIGHT) => +1,\n",
    "        Int(DOWN)  => +3,\n",
    "        Int(LEFT)  => -1,\n",
    "    )\n",
    "    env.state += state_map[action]\n",
    "\n",
    "    # ゴールしたとき報酬 1 を与える\n",
    "    if env.state === Int(S8)\n",
    "        push!(env.state_action_history, (state = Int(S8), action = NaN))\n",
    "        env.reward = 1\n",
    "    end\n",
    "end\n",
    "\n",
    "env = MazeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    pi_theta(theta::Matrix{Number}) = pi::Matrix{Number} (8x4)\n",
    "\n",
    "方策パラメータ θ を行動方策 π に変換する関数\n",
    "\n",
    "- 行動の重み θ から、行動の採用確率 π_θ に変換する\n",
    "- 例: S0 [NaN 1.0 1.0 NaN] の場合\n",
    "    - 行動の採用確率は [0.0 0.5 0.5 0.0] となる\n",
    "\"\"\"\n",
    "pi_theta(theta::Matrix) =\n",
    "    # 各値をその行での割合 (値 / その行の合計値) に変換\n",
    "    ## NaN 値を無視して合計を出すには NaNStatistics.nansum を使うと良い\n",
    "    theta ./ nansum(theta, dims = 2) |>\n",
    "        # NaN 値を 0.0 に変換する\n",
    "        theta -> map(theta) do t isnan(t) ? 0.0 : t end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    pi_theta_softmax(theta::Matrix{Union{Nothing, Number}}) = pi::Matrix{Number} (8x4)\n",
    "\n",
    "方策パラメータ θ を行動方策 π に変換する Softmax 関数\n",
    "\"\"\"\n",
    "pi_theta_softmax(theta::Matrix) = begin\n",
    "    # 逆温度β: 小さいほど行動がランダムになりやすい\n",
    "    beta = 1.0\n",
    "\n",
    "    # exp(βθ)::Matrix{Number} (8x4): ここでマイナス値も正規化される\n",
    "    exp_theta = exp.(beta .* theta)\n",
    "\n",
    "    # π_softmax(θ)\n",
    "    ## 欠損値を無視して行ごとの列値合計を算出するために nansum(::Matrix, dims=2) を使う\n",
    "    pi = exp_theta ./ nansum(exp_theta, dims=2)\n",
    "\n",
    "    # 欠損値を 0.0 に変換して完了\n",
    "    map(pi) do v isnan(v) ? 0.0 : v end\n",
    "end\n",
    "\n",
    "\"方策 π(θ): 方策勾配法\"\n",
    "mutable struct MazePGPolicy <: AbstractPolicy\n",
    "    pi::Matrix{<:Number}\n",
    "end\n",
    "\n",
    "\"現在の状態でエージェントがとる行動を決める方策\"\n",
    "(pi::MazePolicy)(env::MazeEnv) ::Int = wsample(action_space(env), pi.pi[state(env), :])\n",
    "\n",
    "\n",
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
