{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# つくりながら学ぶ！深層強化学習 PyTorchによる実践プログラミング\n",
    "\n",
    "## 迷路問題\n",
    "\n",
    "前回は、価値反復法のアルゴリズムの1つである Sarsa を実装した\n",
    "\n",
    "今回は、価値反復法の別のアルゴリズムである **Q学習** を実装する\n",
    "\n",
    "### Q学習\n",
    "Sarsa の場合、行動価値関数 $Q$ の更新式は以下で表された\n",
    "\n",
    "$$\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + \\eta * (R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "Q学習においては、以下の式で $Q$ は更新される\n",
    "\n",
    "$$\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + \\eta * (R_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "Sarsa の場合は更新時に次の行動 $a_{t+1}$ を求めて更新式に利用していたが、Q学習では状態 $s_{t+1}$ の行動価値関数の値の内最も大きいものを利用する\n",
    "\n",
    "Sarsa のように $Q$ の更新が $a_{t+1}$ を求める方策に依存する特性を **方策オン型** と呼び、Q学習のように $Q$ の更新が方策に依存しない特性を **方策オフ型** と呼ぶ\n",
    "\n",
    "方策オフ型の学習は、ε-greedy法から生成されるランダム性が更新式に入らない分、方策オン型の学習よりも行動価値関数の収束が早くなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×4 Matrix{Float64}:\n",
       " NaN      1.0    1.0  NaN\n",
       " NaN      1.0  NaN      1.0\n",
       " NaN      1.0    1.0    1.0\n",
       "   1.0    1.0    1.0  NaN\n",
       " NaN    NaN      1.0    1.0\n",
       "   1.0  NaN    NaN    NaN\n",
       "   1.0  NaN    NaN    NaN\n",
       "   1.0    1.0  NaN    NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"01_03-maze_module.jl\")\n",
    "\n",
    "theta_0 = Maze.theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NaNStatistics\n",
    "\n",
    "\"\"\"\n",
    "    Q_learning!(Q::Matrix, state::Int, action::Int, reward::Number, state_next::Int; eta = 0.1, gamma = 0.9)\n",
    "        = Q::Matrix\n",
    "\n",
    "Q学習による行動価値関数 Q の更新関数\n",
    "\n",
    "- `Q::Matrix{Number}`: 行動価値関数\n",
    "- `state::Int`: 現在の状態\n",
    "- `action::Int`: 採用する行動\n",
    "- `reward::Number`: 報酬\n",
    "- `state_next::Int`: 行動後の状態\n",
    "- `eta::Number`: 学習率\n",
    "- `gamma::Number`: 時間割引率\n",
    "\"\"\"\n",
    "Q_learning!(Q::Matrix, state::Int, action::Int, state_next::Int; eta = 0.1, gamma = 0.9) = begin\n",
    "    if state_next === 9\n",
    "        # ゴールした場合\n",
    "        Q[state, action] = Q[state, action] + eta * (reward - Q[state, action])\n",
    "    else\n",
    "        Q[state, action] = Q[state, action] + eta * (reward + gamma * nanmaximum(Q[state_next, :]) - Q[state, action])\n",
    "    end\n",
    "    Q\n",
    "end\n",
    "\n",
    "maze!(Q::Matrix, pi_n::Matrix, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
