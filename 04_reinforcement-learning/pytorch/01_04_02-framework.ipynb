{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# つくりながら学ぶ！深層強化学習 PyTorchによる実践プログラミング\n",
    "\n",
    "## 迷路問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `d:\\github\\julia_ml-tuto\\04_reinforcement-learning\\pytorch\\SimpleRL`\n"
     ]
    }
   ],
   "source": [
    "using Revise, Pkg\n",
    "using NaNStatistics, Distributions\n",
    "\n",
    "Pkg.activate(\"./SimpleRL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_theta!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SimpleRL\n",
    "\n",
    "include(\"./inc/retype.jl\")\n",
    "\n",
    "\"\"\"\n",
    "勾配降下法モデル\n",
    "- 強化学習環境 追加インターフェイス:\n",
    "    - 方策パラメータ取得: () -> (方策パラメータ)\n",
    "    - 方策パラメータ設定: (方策パラメータ)!\n",
    "    - 方策マトリクス取得: () -> (方策マトリクス)\n",
    "- 関数:\n",
    "    - 方策パラメータ更新: ()!\n",
    "\"\"\"\n",
    "# Additional Interfaces\n",
    "get_theta(::Env) = error(\"get_theta(::Env) -> (theta::Matrix{<:Number}): Get policy parameters of the environment.\")\n",
    "set_theta!(::Env, theta::Matrix{<:Number}) = error(\"set_theta!(::Env, theta::Matrix{<:Number}): Set policy parameters to the environment.\")\n",
    "get_pi(::Env) = error(\"get_pi(::Env) -> (pi::Matrix{<:Number}): Get policy matrix of the environment.\")\n",
    "\n",
    "\"\"\"\n",
    "    update_theta!(env::Env; eta::AbstractFloat = 0.1)\n",
    "\n",
    "方策勾配法による方策パラメータの更新\n",
    "\n",
    "- `eta::AbstractFloat`: 学習率\n",
    "\"\"\"\n",
    "update_theta!(env::Env; eta::AbstractFloat = 0.1) = begin\n",
    "    records = trajectory(env)()\n",
    "    theta_n = get_theta(env)\n",
    "    pi_n = get_pi(env)\n",
    "\n",
    "    # ゴールまでの総ステップ数: ゴール地点のステップは除外\n",
    "    T = length(records) - 1\n",
    "\n",
    "    # Δθ の計算\n",
    "    delta_theta = [\n",
    "        isnan(theta_n[i, j]) ? NaN : (\n",
    "            begin\n",
    "                # 状態 = s_i である記録を取得\n",
    "                SA_i = filter(SA -> SA.state == i, records)\n",
    "\n",
    "                # 状態 = s_i で 行動 = a_j をとった記録を取得\n",
    "                SA_ij = filter(SA -> SA.state == i && SA.action == j, records)\n",
    "\n",
    "                # N(s_i, a), N(s_i, a_j)\n",
    "                N_i = length(SA_i)\n",
    "                N_ij = length(SA_ij)\n",
    "\n",
    "                # Δθ\n",
    "                (N_ij + pi_n[i, j] * N_i) / T\n",
    "            end\n",
    "        )\n",
    "        for i = 1:size(theta_n, 1), j = 1:size(theta_n, 2)\n",
    "    ]\n",
    "\n",
    "    # θ更新\n",
    "    theta_next = theta_n .+ eta .* delta_theta\n",
    "    set_theta!(env, theta_next)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train!"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ProgressMeter\n",
    "\n",
    "# MazeEnv: 勾配降下法に必要なインターフェイスを追加\n",
    "get_theta(env::MazeEnv) = env.theta\n",
    "set_theta!(env::MazeEnv, theta::Matrix{<:Number}) = SimpleRL.set_theta!(env, theta)\n",
    "get_pi(env::MazeEnv) = env.policy.pi\n",
    "\n",
    "\"\"\"\n",
    "    train!(env::Env; max_epochs::Int = 10_000, stop_epsilon::AbstractFloat = 10^-8) -> (records::Vector{<:NamedTuple})\n",
    "\n",
    "勾配降下法による学習実行\n",
    "\n",
    "- `max_epochs::Int`: 最大学習回数\n",
    "- `stop_epsilon::AbstractFloat`: 学習完了の方策変化しきい値\n",
    "- `records::Vector{<:NamedTuple}`: `[(delta_pi = 方策変化の絶対値和, n_steps = ゴールまでのステップ数)]`\n",
    "\"\"\"\n",
    "train!(env::Env; max_epochs::Int = 10_000, stop_epsilon::AbstractFloat = 10^-8) = begin\n",
    "    # 学習記録: [(方策変化の絶対値和, ゴールまでのステップ数), ...]\n",
    "    records = NamedTuple[]\n",
    "\n",
    "    # 方策勾配法により学習ループする\n",
    "    @showprogress for _ = 1:max_epochs\n",
    "        n_steps = execute!(env) # シナリオ実行\n",
    "\n",
    "        pi_n = get_pi(env)\n",
    "        update_theta!(env) # 方策パラメータ θ の更新\n",
    "        pi_next = get_pi(env)\n",
    "\n",
    "        # 方策変化の絶対値和\n",
    "        delta_pi = sum(abs.(pi_next .- pi_n))\n",
    "        # 記録: (方策変化の絶対値和, ゴールまでのステップ数)\n",
    "        push!(records, (delta_pi = delta_pi, n_steps = n_steps))\n",
    "\n",
    "        # 終了条件\n",
    "        delta_pi < stop_epsilon && break\n",
    "    end\n",
    "\n",
    "    records\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:22:30\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000-element Vector{NamedTuple}:\n",
       " (delta_pi = 0.014882092556085136, n_steps = 14)\n",
       " (delta_pi = 0.020658101693908304, n_steps = 4)\n",
       " (delta_pi = 0.01087033644340879, n_steps = 24)\n",
       " (delta_pi = 0.00998261459094213, n_steps = 10)\n",
       " (delta_pi = 0.006813463110062923, n_steps = 34)\n",
       " (delta_pi = 0.008111677867489675, n_steps = 4)\n",
       " (delta_pi = 0.008857175690487606, n_steps = 20)\n",
       " (delta_pi = 0.006768710649111054, n_steps = 28)\n",
       " (delta_pi = 0.006257468478895023, n_steps = 20)\n",
       " (delta_pi = 0.005863275871530904, n_steps = 26)\n",
       " (delta_pi = 0.005780406388977477, n_steps = 10)\n",
       " (delta_pi = 0.005662528741111839, n_steps = 14)\n",
       " (delta_pi = 0.006067168402568657, n_steps = 6)\n",
       " ⋮\n",
       " (delta_pi = 3.4459950992715704e-7, n_steps = 4)\n",
       " (delta_pi = 3.429859877779329e-7, n_steps = 24)\n",
       " (delta_pi = 3.4746977589650285e-7, n_steps = 12)\n",
       " (delta_pi = 3.5255309976944815e-7, n_steps = 30)\n",
       " (delta_pi = 3.5637635731333717e-7, n_steps = 14)\n",
       " (delta_pi = 3.6612606879593557e-7, n_steps = 16)\n",
       " (delta_pi = 3.6659546998052406e-7, n_steps = 24)\n",
       " (delta_pi = 3.803728370610493e-7, n_steps = 92)\n",
       " (delta_pi = 3.752187576755439e-7, n_steps = 12)\n",
       " (delta_pi = 3.725967229750715e-7, n_steps = 4)\n",
       " (delta_pi = 3.7459121843275156e-7, n_steps = 18)\n",
       " (delta_pi = 3.633157547588972e-7, n_steps = 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 勾配降下法による学習実行\n",
    "env = MazeEnv()\n",
    "records = train!(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7 with sysimage 1.7.2",
   "language": "julia",
   "name": "julia-1.7-with-sysimage-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
