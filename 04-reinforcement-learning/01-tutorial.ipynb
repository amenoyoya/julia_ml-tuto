{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReinforcementLearning.jl\n",
    "\n",
    "https://juliareinforcementlearning.org/docs/tutorial/\n",
    "\n",
    "## 1次元ランダムウォーク\n",
    "\n",
    "```\n",
    "-1     <- @ ->    +1\n",
    " 1  2  3  4  5  6  7\n",
    "```\n",
    "\n",
    "- エージェント:\n",
    "    - 初期位置: 4\n",
    "    - 左か右に一つ動く\n",
    "- ゲームの終了条件:\n",
    "    - エージェントが 1 か 7 に到達すること\n",
    "- 報酬:\n",
    "    - -1: エージェントが 1 に到達\n",
    "    - +1: エージェントが 7 に到達\n",
    "    - 0: 上記以外 (ゲームが終了しないためあり得ない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\user\\.julia\\environments\\v1.7\\Project.toml`\n",
      " \u001b[90m [587475ba] \u001b[39m\u001b[92m+ Flux v0.12.10\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\user\\.julia\\environments\\v1.7\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add([\"ReinforcementLearning\", \"Flux\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# RandomWalk1D\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                Value |\n",
       "|:----------------- | --------------------:|\n",
       "| NumAgentStyle     |        SingleAgent() |\n",
       "| DynamicStyle      |         Sequential() |\n",
       "| InformationStyle  | PerfectInformation() |\n",
       "| ChanceStyle       |      Deterministic() |\n",
       "| RewardStyle       |     TerminalReward() |\n",
       "| UtilityStyle      |         GeneralSum() |\n",
       "| ActionStyle       |   MinimalActionSet() |\n",
       "| StateStyle        | Observation{Int64}() |\n",
       "| DefaultStateStyle | Observation{Int64}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Base.OneTo(7)`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(2)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "4\n",
       "```\n"
      ],
      "text/plain": [
       "# RandomWalk1D\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                Value |\n",
       "|:----------------- | --------------------:|\n",
       "| NumAgentStyle     |        SingleAgent() |\n",
       "| DynamicStyle      |         Sequential() |\n",
       "| InformationStyle  | PerfectInformation() |\n",
       "| ChanceStyle       |      Deterministic() |\n",
       "| RewardStyle       |     TerminalReward() |\n",
       "| UtilityStyle      |         GeneralSum() |\n",
       "| ActionStyle       |   MinimalActionSet() |\n",
       "| StateStyle        | Observation{Int64}() |\n",
       "| DefaultStateStyle | Observation{Int64}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Base.OneTo(7)`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(2)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "4\n",
       "```\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ReinforcementLearning\n",
    "\n",
    "env = RandomWalk1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.OneTo(7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 状態空間: 1..7 まであるため 7\n",
    "S = state_space(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初期位置: 4\n",
    "s = state(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.OneTo(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 行動空間: エージェントは左か右に動くため 2\n",
    "A = action_space(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ゲーム実行\n",
    "while !is_terminated(env)\n",
    "    # エージェントをランダムに動かす\n",
    "    env(rand(A))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(state = 1, reward = -1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    state = state(env),   # 最終位置\n",
    "    reward = reward(env), # 報酬\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 強化学習\n",
    "このゲームでは、エージェントがエピソードで最大の累積報酬を獲得するための最適なポリシーを見つけることを目的とする\n",
    "\n",
    "まずは、まったくのランダムに行動を選択する `RandomPolicy` を使って、10回のエピソードを実行してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
      "            \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
      "          \u001b[38;5;8m1\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⡏\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⢇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⣾\u001b[0m⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡟\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⠈\u001b[0m\u001b[38;5;2m⡆\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⡎\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡸\u001b[0m⠀\u001b[38;5;2m⢱\u001b[0m⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⠸\u001b[0m\u001b[38;5;2m⡀\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⡸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀\u001b[38;5;2m⢇\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "   Score   \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠤⠤\u001b[38;5;2m⡧\u001b[0m⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤\u001b[38;5;2m⢧\u001b[0m⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤\u001b[38;5;2m⡧\u001b[0m⠤⠤⠤\u001b[38;5;2m⢼\u001b[0m⠤⠤\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀⠀⠀\u001b[38;5;2m⠈\u001b[0m\u001b[38;5;2m⡆\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡜\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⠸\u001b[0m\u001b[38;5;2m⡀\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⠘\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡜\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡸\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢣\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "         \u001b[38;5;8m-1\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢱\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "            \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
      "            ⠀\u001b[38;5;8m1\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m10\u001b[0m⠀ \n",
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TotalRewardPerEpisode([-1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0], 0.0, true)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\n",
    "    RandomPolicy(),\n",
    "    RandomWalk1D(),\n",
    "    StopAfterEpisode(10),\n",
    "    TotalRewardPerEpisode()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の例では、エージェントはランダムに行動するため、報酬結果もランダムとなる\n",
    "\n",
    "一方 `TabularPolicy` を使うと、事前にエージェントの行動を設定しておくことができる\n",
    "\n",
    "以下の例では、エージェントに毎回「右に移動する」という行動ポリシーを与えているため、報酬結果は常に +1 となる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
      "           \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
      "         \u001b[38;5;8m2\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "   Score  \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "         \u001b[38;5;8m0\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
      "           ⠀\u001b[38;5;8m1\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m10\u001b[0m⠀ \n",
      "           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TotalRewardPerEpisode([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.0, true)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事前設定行動ポリシー: table = Dict(状態 => 行動)\n",
    "## <RandomWalk1D>\n",
    "## - 状態空間 = エージェントの位置: 1, 2, 3, 4, 5, 6, 7\n",
    "## - 行動空間: 1 => 左に移動, 2 => 右に移動\n",
    "policy = TabularPolicy(; table = Dict(\n",
    "    1 => 2, # エージェント位置が 1 のとき、右に移動\n",
    "    2 => 2, # エージェント位置が 2 のとき、右に移動\n",
    "    3 => 2, # エージェント位置が 3 のとき、右に移動\n",
    "    4 => 2, # エージェント位置が 4 のとき、右に移動\n",
    "    5 => 2, # エージェント位置が 5 のとき、右に移動\n",
    "    6 => 2, # エージェント位置が 6 のとき、右に移動\n",
    "    7 => 2, # エージェント位置が 7 のとき、右に移動\n",
    "))\n",
    "\n",
    "run(\n",
    "    policy,\n",
    "    RandomWalk1D(),\n",
    "    StopAfterEpisode(10),\n",
    "    TotalRewardPerEpisode()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでのポリシーは、エージェントにランダム行動をとらせるか、事前設定した行動をとらせていた\n",
    "\n",
    "しかしこのゲームの目的は、自動的にエージェントが最適な行動をとるように学習させることである\n",
    "\n",
    "このような学習を **強化学習** と呼ぶ\n",
    "\n",
    "原始的な強化学習として最も有名なのは **Q学習** であり、ReinforcementLearning.jl では `QBasedPolicy` として提供されている\n",
    "\n",
    "このポリシーには「各状態における行動の価値を推定する状態アクション値関数 (`learner`)」と「推定した状態アクション値の結果に基づいて実行する行動を選択するエクスプローラ (`explorer`)」の2つの処理系が含まれている\n",
    "\n",
    "以下の例では、learner に `MonteCarloLearner`, explorer に `EpsilonGreedyExplorer` を使って、Q学習ポリシーを構成している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(QBasedPolicy)\n",
       "├─ learner => typename(MonteCarloLearner)\n",
       "│  ├─ approximator => typename(TabularApproximator)\n",
       "│  │  ├─ table => 2×7 Matrix{Float64}\n",
       "│  │  └─ optimizer => typename(InvDecay)\n",
       "│  │     ├─ gamma => 1.0\n",
       "│  │     └─ state => typename(IdDict)\n",
       "│  ├─ γ => 1.0\n",
       "│  ├─ kind => typename(ReinforcementLearningZoo.FirstVisit)\n",
       "│  └─ sampling => typename(ReinforcementLearningZoo.NoSampling)\n",
       "└─ explorer => typename(EpsilonGreedyExplorer)\n",
       "   ├─ ϵ_stable => 0.1\n",
       "   ├─ ϵ_init => 1.0\n",
       "   ├─ warmup_steps => 0\n",
       "   ├─ decay_steps => 0\n",
       "   ├─ step => 1\n",
       "   ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "   └─ is_training => true\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: InvDecay\n",
    "\n",
    "policy = QBasedPolicy(;\n",
    "    learner = MonteCarloLearner(;\n",
    "        approximator = TabularQApproximator(;\n",
    "            n_state = length(S),\n",
    "            n_action = length(A),\n",
    "            opt = InvDecay(1.0)\n",
    "        )\n",
    "    ),\n",
    "    explorer = EpsilonGreedyExplorer(0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記で構成したポリシーは最適化されていないため、そのまま実行しても良い報酬を得ることはできない\n",
    "\n",
    "ポリシーを最適化するためには、ラッパーポリシー `Agent` を用いて、学習モードでゲームを実行する必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
      "            \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
      "          \u001b[38;5;8m1\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⡏\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⡎\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⡸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "   Score   \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠤⠤\u001b[38;5;2m⡧\u001b[0m⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡜\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "         \u001b[38;5;8m-1\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "            \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
      "            ⠀\u001b[38;5;8m1\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m10\u001b[0m⠀ \n",
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TotalRewardPerEpisode([-1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.0, true)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy を学習モードで実行するための Agent ポリシー\n",
    "agent = Agent(\n",
    "    policy = policy,\n",
    "\n",
    "    # VectorSARTTrajectory: アクション, 報酬, 状態を保存する\n",
    "    ## => 上記のデータを元に学習を行う\n",
    "    trajectory = VectorSARTTrajectory()\n",
    ")\n",
    "\n",
    "run(\n",
    "    agent,\n",
    "    RandomWalk1D(),\n",
    "    StopAfterEpisode(10),\n",
    "    TotalRewardPerEpisode()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は単純な問題であるため、少々わかりにくいが、おそらく最初の1～2回はランダムに行動している\n",
    "\n",
    "それ以降で学習が上手く進み、報酬が +1 で安定するようになっていると思われる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
